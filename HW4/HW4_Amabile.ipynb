{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 4 - ARNO AMABILE\n",
    "___\n",
    "# Assignment 1 - Distributions comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "\n",
    "#this makes my plots pretty! but it is totally not mandatory to do it\n",
    "import json\n",
    "s = json.load( open(os.getenv ('PUI2015')+\"/fbb_matplotlibrc.json\") )\n",
    "pl.rcParams.update(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I am reading in data from January 2015. It would be a good idea to use data from warmer months, since there are more riders in the warm weather and the more data the smaller the STATISTICAL NOISE. If you are ambitios you can use data from multiple months, thus addressing systematic errors as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(os.getenv ('PUI2015')+'/notebooks/data/201501-citibike-tripdata.csv')\n",
    "print df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df is the dataframe where the content of the csv file is stored\n",
    "df['ageM'] = 2015-df['birth year'][(df['usertype'] == 'Subscriber') & (df['gender'] == 1)]\n",
    "df['ageF'] = 2015-df['birth year'][(df['usertype'] == 'Subscriber') & (df['gender'] == 2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets splot age in 10 year bins. the dataset is very large, so i could be split in smaller bins, but I will chose 10 years in the interest of time. if i was to do this \"for real\" the bin size choice should be a balance between properly sample the age space, have enough counts in each bin that the statistical noise is not significant (remember that is > sqrt(N)!) and the computational requirement to computatinal facilities ratio.\n",
    "the next several steps are needed if you want to code up the KS test from scratch. that is for extra credit, so if you do not want to do it you may not need to plot split the distribution in bins and create the cumulative HOWEVER it is a great idea to do it anyways to explore your data viaually! remember Ascombe's quartet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = np.arange(10, 99, 10)\n",
    "df.ageM.groupby(pd.cut(df.ageM, bins)).agg([count_nonzero]).plot(kind='bar')\n",
    "df.ageF.groupby(pd.cut(df.ageF, bins)).agg([count_nonzero]).plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is how the cumulative distributions look like. Notice that i am normalizing them! if i want to reat an observed distribution like a probablility distribution i have to normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print df.ageS, df.ageS.cumsum()\n",
    "\n",
    "csM=df.ageM.groupby(pd.cut(df.ageM, bins)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "csF=df.ageF.groupby(pd.cut(df.ageF, bins)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "print np.abs(csM / csM.max()-csF / csF.max())\n",
    "\n",
    "pl.plot(bins[:-1] + 5, csM / csM.max(), label = \"M\")\n",
    "pl.plot(bins[:-1] + 5, csF / csF.max(), label = \"F\")\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "they look pretty darn similar! that sets my expectation for the tests to come. if i wanted to code the KS test by hand i woud have everything i need: the normalized cumulative distributions can be subtracted from each other and the max distance can be seeked.\n",
    "Notice that there amay be Nan values you are gonna have to deal with! you can do that for example with a Boolean statementsuch as df.ageF[~np.isnan(df.ageF)] or you can use numpy functions that deal with Nan values: nansum, nanmean, nanstd...\n",
    "lets run the scipy KS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KS Test to compare 2 samples\n",
    "http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ks_2samp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks=scipy.stats.ks_2samp(df.ageM, df.ageF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remember: the Null hypothesis is rejected if\n",
    "$D_KS(n1,n2)>c(α)\\sqrt{\\frac{n1+n2}{n1n2}}$\n",
    "where c(α) is the inverse of the KS distribution, and you do not have to know how to get that cause there are tables that list critical values!!\n",
    "http://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/kolmogorov-smirnov-test/kolmogorov-distribution/\n",
    "but scipy is already giving you the p-value, s you do not have to worry about it i am still giving you the table FWI and if you want to do the coding of the KS test by hand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#here is the critical values tablel. Have you chosen your significance level yet?? you should do it first thing!\n",
    "from IPython.display import Image\n",
    "Image(filename=\"ks2sample_table.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pearson's test for correlation\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr\n",
    "\n",
    "Spearman's test for correlation\n",
    "http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr\n",
    "\n",
    "# please perform the Spearman's test and tell me what you find\n",
    "please comment on the numbers that you get in the light of the scipy manual: what is rerutned? what does it mean??\n",
    "DO THE SAME FOR THE DISTRIBUTION OF RIDERS DURING THE DAY VS DURING THE NIGHT!\n",
    "\n",
    "hint: remember how I converted the starttime last time in my own citibikes notebook into units that are easy to read. df['mystarttime'] = ... then you can use something like df['mystarttime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Goodness of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rdn\n",
    "%pylab inline\n",
    "\n",
    "import os\n",
    "\n",
    "#this makes my plots pretty! but it is totally not mandatory to do it\n",
    "import json\n",
    "s = json.load( open(os.getenv ('PUI2015')+\"/fbb_matplotlibrc.json\") )\n",
    "pl.rcParams.update(s)\n",
    "\n",
    "#plus importing scipy.stats\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Test of Binomial against Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate the distribution\n",
    "N = 1000\n",
    "np.random.seed(1122)\n",
    "dist_n = np.random.randn(N)\n",
    "\n",
    "alpha = 0.1\n",
    "if alpha == 0.01:\n",
    "    threshold_ks = 1.63/sqrt(N) \n",
    "elif alpha == 0.05:\n",
    "    threshold_ks = 1.36/sqrt(N)\n",
    "elif alpha >0.05:\n",
    "    print \"Why such low standards? We need to be more ambitious!\\n\"\n",
    "    alpha = 0.05\n",
    "    threshold_ks = 1.36/sqrt(N)\n",
    "else:\n",
    "    print \"Why such weird standards? We cannot afford originality!\\n\"\n",
    "    alpha = 0.05\n",
    "    threshold_ks = 1.36/sqrt(N)\n",
    "\n",
    "print \"AD Test of Normal distribution on Gaussian:\", \\\n",
    "\"\\n - Critical values: \", scipy.stats.anderson(dist_n, dist='norm')[1], \\\n",
    "\"\\n - Significance levels:\", scipy.stats.anderson(dist_n, dist='norm')[2]    \n",
    "    \n",
    "threshold_ad = np.asscalar(scipy.stats.anderson(dist_n, dist='norm')[1]\n",
    "                           [scipy.stats.anderson(dist_n, dist='norm')[2]==[alpha*100]])\n",
    "threshold_kl = -log(alpha)/N\n",
    "\n",
    "print \"\\n Critical values: KS = {0:.3f}; AD = {1:.3f}; KL = {2:.3f} \\n\".format(threshold_ks, threshold_ad, threshold_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array of parameters n for the binomial distributions\n",
    "np.random.seed(2415)\n",
    "narray = sorted(rdn.sample(xrange(10,1000), 50))\n",
    "\n",
    "# Create arrays of zero to fill with results\n",
    "ks_b = np.zeros(len(narray))\n",
    "ad_b = np.zeros(len(narray))\n",
    "kl_b = np.zeros(len(narray))\n",
    "\n",
    "# Definition of the normal distribution function to be called in KS\n",
    "def mynorm (x, mu, var):\n",
    "    return scipy.stats.norm.cdf(x, loc=mu, scale=var)\n",
    "\n",
    "p=0.6 # Choice of p parameter, common to all binomial distributions\n",
    "\n",
    "for i,n in enumerate(narray):\n",
    "    # Generate the distribution\n",
    "    np.random.seed(4216)\n",
    "    dist_b = np.random.binomial(n, p, N)\n",
    " \n",
    "    # Run the tests\n",
    "    ks_b[i] = scipy.stats.kstest(dist_b, mynorm, args=(n*p, n*p*(1.0-p)))[0]\n",
    "    ad_b[i] = scipy.stats.anderson(dist_b, dist='norm')[0]\n",
    "    \n",
    "    # For KL, I need to simulate the normal distribution as well\n",
    "    mybins=np.linspace(min(dist_b),max(dist_b), 10) \n",
    "    bincenters = mybins[:-1]+0.5*(mybins[1]-mybins[0])\n",
    "\n",
    "    kl_b [i] =  scipy.stats.entropy(np.histogram(dist_b, bins=mybins)[0], \n",
    "                                    scipy.stats.norm.pdf(bincenters, loc=n*p, scale=n*p*(1.0-p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize = (20,5))\n",
    "\n",
    "fig.add_subplot(131)\n",
    "pl.plot(narray, ks_b, label='KS')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_ks, threshold_ks], label=\"Threshold\")\n",
    "pl.xlabel(\"Parameter n\")\n",
    "pl.legend(loc=2)\n",
    "\n",
    "fig.add_subplot(132)\n",
    "pl.plot(narray, ad_b,  label='AD')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_ad, threshold_ad], label=\"Threshold\")\n",
    "pl.xlabel(\"Parameter n\")\n",
    "pl.legend(loc=2)\n",
    "\n",
    "fig.add_subplot(133)\n",
    "pl.plot(narray, kl_b, label='KL ')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_kl, threshold_kl], label=\"Threshold\")\n",
    "pl.xlabel(\"Parameter n\")\n",
    "pl.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KS, AD, and KL statistics with increases of n.** Tests of divergence between samples ($N=1000$) from binomial distribution (parameters $p=0.6$ and $n$) and $\\mathcal{N}(\\mu=$ np$,\\sigma=$ np(1-p)$)$. **Two effects: (1)** When n increases, the divergence with the normal decreases, hence the AD statistic under the threshold (fail to reject $H_0$) when n is more than 500. **(2)** As N is large, the thresholds for KS and KL are very low: even if the binomial distribution tends to the normal, the KS and KL tests \"recognize\" that it is not the same. The threshold for AD does not depend on N, hence the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Test of Poisson against Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create array of parameters n for the binomial distributions\n",
    "np.random.seed(2415)\n",
    "larray = sorted(rdn.sample(xrange(1,1000), 50))\n",
    "\n",
    "# Create arrays of zero to fill with results\n",
    "ks_p = np.zeros(len(narray))\n",
    "ad_p = np.zeros(len(narray))\n",
    "kl_p = np.zeros(len(narray))\n",
    "\n",
    "# Definition of the normal distribution function to be called in KS\n",
    "def mynorm (x, mu, var):\n",
    "    return scipy.stats.norm.cdf(x, loc=mu, scale=var)\n",
    "\n",
    "for i,lam in enumerate(narray):\n",
    "    # Generate the distribution\n",
    "    np.random.seed(2118)\n",
    "    dist_p = np.random.poisson(lam, N)\n",
    " \n",
    "    # Run the tests\n",
    "    ks_p[i] = scipy.stats.kstest(dist_p, mynorm, args=(lam, lam))[0]\n",
    "    ad_p[i] = scipy.stats.anderson(dist_p, dist='norm')[0]\n",
    "    \n",
    "    # For KL, I need to simulate the normal distribution as well\n",
    "    mybins = np.linspace(min(dist_p),max(dist_p), 10) \n",
    "    bincenters = mybins[:-1]+0.5*(mybins[1]-mybins[0])\n",
    " \n",
    "    kl_p[i] = scipy.stats.entropy(np.histogram(dist_p, bins=mybins)[0],\n",
    "                                  scipy.stats.norm.pdf(bincenters, loc=lam, scale=lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize = (20,5))\n",
    "\n",
    "fig.add_subplot(131)\n",
    "pl.plot(narray, ks_b, label='KS')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_ks, threshold_ks])\n",
    "pl.xlabel(r'Parameter $\\lambda$')\n",
    "pl.legend(loc=2)\n",
    "\n",
    "fig.add_subplot(132)\n",
    "pl.plot(narray, ad_b,  label='AD')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_ad, threshold_ad])\n",
    "pl.xlabel(r'Parameter $\\lambda$')\n",
    "pl.legend(loc=2)\n",
    "\n",
    "fig.add_subplot(133)\n",
    "pl.plot(narray, kl_b, label='KL ')\n",
    "pl.plot([narray[0], narray[-1]],[threshold_kl, threshold_kl])\n",
    "pl.xlabel(r'Parameter $\\lambda$')\n",
    "pl.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KS, AD, and KL statistics with increases of $\\lambda$.** Tests of divergence between samples ($N=1000$) from Poisson distribution (parameter $\\lambda$) and $\\mathcal{N}(\\mu=\\lambda,\\sigma=\\lambda)$. **Two effects: (1)** When $\\lambda$ increases, the divergence with the normal decreases, hence the AD statistic under the threshold (fail to reject $H_0$) when n is more than 500. **(2)** As N is large, the thresholds for KS and KL are very low: even if the Poisson distribution tends to the normal, the KS and KL tests \"recognize\" that it is not the same. The threshold for AD does not depend on N, hence the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = lambda x, mu : scipy.stats.distributions.poisson.pmf(x,mu)\n",
    "n = lambda x, mu : scipy.stats.distributions.norm.pdf(x-mu)\n",
    "c = lambda x, mu : scipy.stats.distributions.chi2.pdf(x, df=mu)\n",
    "f = lambda x, mu : scipy.stats.distributions.f.pdf(x,100,100)\n",
    "\n",
    "Dkl = lambda x ,mu, p :  p(x, mu)* np.log10(n(x, mu)) + p(x, mu) *np.log10(p(x, mu))\n",
    "\n",
    "# Choosing parameters: the Poisson distribution needs another support x_p, as it requires integers\n",
    "mu = 2\n",
    "x=np.linspace(0,10,100)\n",
    "x_p=np.arange(0,10,1)\n",
    "\n",
    "pl.figure(figsize=(15,10))\n",
    "\n",
    "pl.plot(x, n(x,mu), label='Normal')\n",
    "pl.plot(x_p, p(x_p,mu), label='Poisson')\n",
    "pl.plot(x_p, Dkl(x_p, mu, p), '-.', label = 'KL - Poisson')\n",
    "\n",
    "pl.plot(x, c(x,mu), label='Chi-sq')\n",
    "pl.plot(x,Dkl(x, mu, c), '--', label = 'KL - Chi-sq')\n",
    "\n",
    "pl.plot(x, f(x,mu), label='F')\n",
    "pl.plot(x,Dkl(x, mu, f), '--', label = 'KL - F')\n",
    "\n",
    "pl.xlim(0,10)\n",
    "pl.ylabel(\"P(k) and KL Divergence\")\n",
    "pl.legend( fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kullback-Leibler Divergence statistic for several distributions.** The Divergence is a measure of the \"distance\" between a point in the distribution and the corresponding point from the Normal distribution with same mean and same variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
